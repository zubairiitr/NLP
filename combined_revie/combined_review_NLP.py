# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B2-1TTcnrBdcF6sY02xd6eaqHb8AmezS
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""1) Importing the data set"""

data_yelp = pd.read_csv('yelp_labelled.txt', sep = '\t', header = None)

data_yelp.head()

# Assign Column names 
column_name = ['Review', 'Sentiment']
data_yelp.columns = column_name

data_amazon = pd.read_csv('amazon_cells_labelled.txt',sep = '\t', header = None)

data_amazon.columns = column_name

data_amazon.head()

data_imdb = pd.read_csv('imdb_labelled.txt',sep ='\t', header = None)

data_imdb.columns = column_name

data_amazon.shape

data_imdb.head()

data_imdb.head()

# Append all the data in a single dataframe

data = data_yelp.append([data_amazon,data_imdb], ignore_index = True)

data.shape

data.head()

data.tail()

#cheack distribution of sentiments

data['Sentiment'].value_counts

# check for null values

data.isnull().sum

x = data['Review']

y = data['Sentiment']

""" 2 Data **Cleaning**"""

# here we will remove stopword, puntuations
# as well as we will apply lemmtization

"""Creat a function to clean the data"""

import string

punct = string.punctuation

punct

from spacy.lang.en.stop_words import STOP_WORDS

stopwords = list(STOP_WORDS) # list of stopwords

#stopwords

# craeting a function for data cleaniing

import spacy
nlp = spacy.load('en_core_web_sm')

def  text_data_cleaning(sentence):
  doc = nlp(sentence)
  tokens = [] # list of tokens
  for token in doc:
    if token.lemma_!= "-PRON-":
      temp = token.lemma_.lower().strip()
    else:
      tem = token.lower_
    tokens.append(temp)

  cleaned_tokens = []
  for token in tokens:
      if token not in stopwords and token not in punct:
        cleaned_tokens.append(token)
  return cleaned_tokens

"""if root form of that word is not pronoun then it is going to convert that into lower form.
and if that word is a proper noun, then we are directly taking lower form, becoz there is no  lemmma
"""

(text_data_cleaning("Hello all, It's a beutiful day outside there!"))

"""Vectorization Feature Engineering (TFIDF)"""

from sklearn.svm import LinearSVC
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline

tfidf = TfidfVectorizer(tokenizer = text_data_cleaning)
# tokenizer = text_data_cleaning, tokenization will be done according to this function

classifier = LinearSVC()

"""3) Train the model

Splitting the dataset into Train and Test
"""

from sklearn.model_selection import train_test_split
x_train, x_text, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state = 0 )

x_train.shape, x_text.shape

x_train.head()

"""Fit the x_train and y_train"""

clf = Pipeline([('tfidf', tfidf),('clf',classifier)])

clf.fit(x_train, y_train)

"""Predict the test set results"""

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

y_pred = clf.predict(x_text)

# confusion_matix 
confusion_matrix(y_test, y_pred)

# classification report 
print(classification_report(y_test, y_pred))
# we are getting almost 77% accurcy

accuracy_score(y_test, y_pred)
# 76# accuracy

clf.predict(['wow, i am learning  Natural Language Processing in the fashion!'])
# output is 1, that means review is positive

clf.predict(["It's hard to learn new things"])
# output is 0, that means review is negative