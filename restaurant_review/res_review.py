# -*- coding: utf-8 -*-
"""res_review.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10-lp_Z5oAlTB-OAhZ0upVQtoCqFUEbF8

The objective of this project is to utilize machine learning to analyze comments text and evaluate the restaurant business model.
"""

import matplotlib.pyplot as plt
import pandas as pd

data = pd.read_csv("Restaurant_Reviews.tsv", sep="\t", quoting = 3) # read .tsv file

data.head() # display first five record

data.tail() # display last five record

data['Liked'].value_counts() # total number of positive and negative comments. here data is quite balanc

"""Cleaning Text Data"""

import nltk
import re

"""In Natural Language Processing (NLP), stopwords are words that are commonly used in a language but do not add significant meaning to a sentence. These words are often removed from text data because they do not help with text analysis and may even introduce noise or bias. Examples of stopwords in English include "a", "an", "the", "and", "or", "in", "on", "of", "to", etc."""

nltk.download("stopwords")

from nltk.corpus import stopwords

"""In Natural Language Processing (NLP), a corpus refers to a large and structured collection of text data that is used for language analysis, model training, and testing."""

data["Review"][0]

review = re.sub('[^a-zA-Z]', ' ', data['Review'][0])

""" is a regular expression operation that replaces all non-alphabetic characters in the first review text of a Pandas DataFrame object data with a space character."""

review

review.lower()

review = review.split()
review

stopwords.words('english')

preview = []
for word in review:
  if word not in stopwords.words('english'):
     preview.append(word)

preview

preview

"""The Porter stemming algorithm is a widely used algorithm for stemming English words in Natural Language Processing (NLP). Stemming is the process of reducing a word to its base or root form, by removing suffixes and prefixes, and mapping related words to the same stem."""

from nltk.stem.porter import PorterStemmer 
ps = PorterStemmer()

preview = [ps.stem(word)for word in preview]

preview = " ".join(preview)

preview

corpus = []
ps = PorterStemmer()
for i in range(len(data)):
  review = re.sub('[^a-zA-Z]', ' ', data['Review'][i])
  review = review.lower()
  review = review.split()
  review = [ps.stem(word) for word in review if word not in stopwords.words("english")]
  review = " ".join(review)
  corpus.append(review)

print(corpus)

"""In Natural Language Processing (NLP), the Bag of Words (BoW) model is a commonly used technique for representing text data as a vector of numerical features that can be used for machine learning algorithms.

Bag of Word Model
"""

from sklearn.feature_extraction.text import CountVectorizer

"""CountVectorizer is a class in the Python scikit-learn library used to convert a collection of text documents into a matrix of token counts.
The CountVectorizer class performs several preprocessing steps on the input text data, including tokenization, lowercasing, and stop word removal, to create a vocabulary of unique words. It then transforms each document into a sparse matrix, where each row represents a document and each column represents a word in the vocabulary. The values in each cell represent the count of how many times that word appears in the corresponding document
"""

cv = CountVectorizer(max_features = 1500)

x = cv.fit_transform(corpus).toarray()

x.shape

y = data.iloc[:,1].values

y.shape

y[:10]

"""Apply Naive Bayes Alorithm"""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.20, random_state = 0)

x_train.shape, x_test.shape

y_train.shape, y_test.shape

from sklearn.naive_bayes import GaussianNB

classifier = GaussianNB()

classifier.fit(x_train, y_train)

y_pred = classifier.predict(x_test)

from sklearn.metrics import accuracy_score

accuracy_score(y_test,y_pred)

200*0.73