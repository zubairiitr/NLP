# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JQxjHT1XNcktOHFv8vM1TQCi_zwsKnkU

Tex Summarization: In this exersie we will see very basic idea of automated 
text summarization process.
"""

text = """Maria Yuryevna Sharapova (Russian: Мария Юрьевна Шарапова, 
pronounced [mɐˈrʲijə ʂɐˈrapəvə] (listen); born 19 April 1987) is a Russian 
former world No. 1 tennis player. She competed on the WTA Tour from 2001 to 
2020 and was ranked world No. 1 in singles by the Women's Tennis Association 
(WTA) for 21 weeks. She is one of ten women, and the only Russian, to achieve 
the career Grand Slam. She is also an Olympic medalist, having won silver in 
women's singles at the 2012 London Olympics.
Sharapova became the world No. 1 for the first 
time on 22 August 2005 at the age of 18, becoming t
he first Russian woman to top the singles rankings, and 
last held the position for a fifth time for four weeks 
from 11 June 2012 to 8 July 2012.[4][5] She won five major titles — two at the
 French Open and one each at the Australian Open, Wimbledon, and the US Open. She won 
 36 titles in total, including the year-end championships in her debut in 2004. She also won 
 three doubles titles. Although she played under the banner of Russia with the WTA, she has lived in and been a 
 United States permanent resident since 1994.[6]"""

"""1) Importing the libraries and Data set"""

import spacy
from spacy.lang.en.stop_words import STOP_WORDS
from string import punctuation

nlp = spacy.load("en_core_web_sm")

doc = nlp(text)

token = [token.text for token in doc]
print(token)

punctuation

punctuation = punctuation + '\n'
punctuation

"""2) Text Cleaning """

word_freq = {}

for word in doc:
    stop_words = list(STOP_WORDS)
    if word.text.lower() not in stop_words:
      if word.text.lower() not in punctuation:
        if word.text not in word_freq.keys():
          word_freq[word.text] = 1
        else:
          word_freq[word.text] += 1
print(word_freq)

max_freq = max(word_freq.values())

for word in word_freq.keys():
  word_freq[word] = word_freq[word]/max_freq
print(word_freq)

"""3) Sentance tokenization"""

sent_tokens =[sent for sent in doc.sents]
print(sent_tokens)

sent_score  = {}

for sent in sent_tokens:
  for word in sent:
    if word.text.lower() in word_freq.keys():
      if sent not in sent_score.keys():
        sent_score[sent] = word_freq[word.text.lower()]
      else:
        sent_score[sent] += word_freq[word.text.lower()]
    
print(sent_score)

"""4) Select 30% sentance with maximum score """

from heapq import nlargest

len(sent_score)*0.3

"""5) Getting the summary"""

summary = nlargest(n= 3, iterable = sent_score, key = sent_score.get)

print(summary)

final_summary = [word.text for word in  summary]

print(final_summary)

summary = " ". join(final_summary)

print(summary)

len(summary)/len(text)
# so almost 40 % data is gather and almost 60% data is reject

